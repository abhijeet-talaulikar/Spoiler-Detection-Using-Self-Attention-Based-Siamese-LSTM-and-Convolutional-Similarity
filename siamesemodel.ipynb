{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7KypBlO_fw5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, roc_auc_score\n",
        "import swifter\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import tensorflow as tf \n",
        "\n",
        "print(tf. __version__)\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import visualkeras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8ZEZMn0_fw6"
      },
      "source": [
        "### Load and Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FtHuKTg_zxQ"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"splr.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"data\")\n",
        "\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"glove.6B.50d.txt.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFbipuWT_fw8"
      },
      "outputs": [],
      "source": [
        "df_reviews = pd.read_json('data/IMDB_reviews.json', lines=True)\n",
        "df_details = pd.read_json('data/IMDB_movie_details.json', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews.is_spoiler.value_counts()"
      ],
      "metadata": {
        "id": "x-Kb8HTviRE4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9NwSpfA_fw8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df = pd.merge(df_reviews[['movie_id','review_text','is_spoiler']], df_details[['movie_id','plot_summary']], on='movie_id')\n",
        "df = pd.concat([df[df.is_spoiler].sample(frac=1, replace=True), df[~df.is_spoiler].sample(frac=0.35)]).sample(frac=1).reset_index(drop=True)\n",
        "#df = df.groupby('is_spoiler', group_keys=False).apply(lambda x: x.sample(frac=0.5))\n",
        "df['is_spoiler'] = (df['is_spoiler'] == True).astype(int)\n",
        "df = df[['review_text','plot_summary','is_spoiler']]\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.is_spoiler.value_counts()"
      ],
      "metadata": {
        "id": "ttrYluDK-kj8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM7UmiHU_fw9"
      },
      "outputs": [],
      "source": [
        "sw = stopwords.words('english')\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def preprocess_text(review):\n",
        "    review = review.lower() # Convert to lowercase\n",
        "    review = re.sub('[^a-zA-Z]',' ', review) # Remove words with non-letter characters\n",
        "    words = review.split()\n",
        "    words = [word for word in words if word not in sw] # Remove stop words\n",
        "    review = \" \".join(words)\n",
        "    return review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzji75WY_fw-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df['review_text'] = df['review_text'].swifter.apply(preprocess_text)\n",
        "df['plot_summary'] = df['plot_summary'].swifter.apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2WO431b_fw_"
      },
      "source": [
        "### Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL2QvMMu_fxA"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.25, stratify=df['is_spoiler'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ85y0R5_fxB"
      },
      "source": [
        "### Create text vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRg7nSS9_fxB"
      },
      "outputs": [],
      "source": [
        "review_lens = [len(sentence.split()) for sentence in train_df['review_text']]\n",
        "plot_lens = [len(sentence.split()) for sentence in train_df['plot_summary']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEh6L3bP_fxB"
      },
      "outputs": [],
      "source": [
        "train_sentences = list(pd.concat([train_df['review_text'], train_df['plot_summary']]))\n",
        "train_reviews = list(train_df['review_text'])\n",
        "train_plots = list(train_df['plot_summary'])\n",
        "train_labels = list(train_df['is_spoiler'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Giw85zLZ_fxC"
      },
      "outputs": [],
      "source": [
        "val_reviews = list(val_df['review_text'])\n",
        "val_plots = list(val_df['plot_summary'])\n",
        "val_labels = list(val_df['is_spoiler'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3bqWqlv_fxC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "avg_review_len, avg_plot_len = np.mean(review_lens), np.mean(plot_lens)\n",
        "avg_review_len, avg_plot_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFR6ORa3_fxD"
      },
      "outputs": [],
      "source": [
        "max_tokens = 50000\n",
        "output_seq_len = int(max(avg_review_len, avg_plot_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-6C84te_fxD"
      },
      "outputs": [],
      "source": [
        "del df_reviews\n",
        "del df_details\n",
        "del df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qj7njJx_fxD"
      },
      "outputs": [],
      "source": [
        "text_vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=output_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXcdJwxZ_fxE"
      },
      "outputs": [],
      "source": [
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wI2MWLN_fxE"
      },
      "outputs": [],
      "source": [
        "text_vocab = text_vectorizer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE32gIs8If6B"
      },
      "outputs": [],
      "source": [
        "del train_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tXZCsAc_fxE"
      },
      "source": [
        "### Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV9sxM61_fxE"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(((train_reviews, train_plots), train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(((val_reviews, val_plots), val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsG-kfc8_fxF"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(512).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExikvIiz_fxF"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdXm7pV9_fxF"
      },
      "outputs": [],
      "source": [
        "class TextCNN(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 token_embed,\n",
        "                 text_vectorizer,\n",
        "                 kernel_sizes,\n",
        "                 dropout_rate=0.4,\n",
        "                 dilation_rate=1\n",
        "                 ):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.token_embed = token_embed\n",
        "        self.text_vectorizer = text_vectorizer\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.convs = []\n",
        "        \n",
        "        self.bi_lstm = layers.Bidirectional(layers.LSTM(16, return_sequences=True, return_state=False))\n",
        "\n",
        "        for i, kernel_size in enumerate(self.kernel_sizes):\n",
        "            self.convs.append(layers.Conv1D(16, kernel_size, activation='relu', dilation_rate=dilation_rate))\n",
        "        \n",
        "        self.max_pooling = layers.GlobalMaxPooling1D()\n",
        "        \n",
        "        self.classifier = tf.keras.Sequential(\n",
        "                    [\n",
        "                      layers.Dropout(dropout_rate),\n",
        "                      layers.Dense(64, activation=\"relu\"),\n",
        "                      layers.Dropout(dropout_rate),\n",
        "                      layers.Dense(64, activation=\"relu\")\n",
        "                    ]\n",
        "                  )\n",
        "        \n",
        "        self.attention = layers.Attention(use_scale=True)\n",
        "        \n",
        "        self.similarity = tf.keras.Sequential(\n",
        "                    [\n",
        "                      layers.Dot(axes=1, normalize=True),\n",
        "                      layers.Dense(1, activation=\"sigmoid\")\n",
        "                    ]\n",
        "                  )\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        review_inputs, plot_inputs = inputs\n",
        "        \n",
        "        review_vectors = self.text_vectorizer(review_inputs)\n",
        "        plot_vectors = self.text_vectorizer(plot_inputs)\n",
        "        \n",
        "        review_embeddings = self.token_embed(review_vectors)\n",
        "        plot_embeddings = self.token_embed(plot_vectors)\n",
        "        \n",
        "        review_embeddings_bilstm = self.bi_lstm(review_embeddings)\n",
        "        plot_embeddings_bilstm = self.bi_lstm(plot_embeddings) \n",
        "        \n",
        "        r = layers.Dropout(0.2)(review_embeddings_bilstm)\n",
        "        for i, k in enumerate(self.kernel_sizes):\n",
        "            r = self.convs[i](r)\n",
        "        \n",
        "        r = self.max_pooling(r)\n",
        "        r = self.attention([r,r])\n",
        "        r = self.classifier(r)\n",
        "        \n",
        "        p = layers.Dropout(0.2)(plot_embeddings_bilstm)\n",
        "        for i, k in enumerate(self.kernel_sizes):\n",
        "            p = self.convs[i](p)\n",
        "        \n",
        "        p = self.max_pooling(p)\n",
        "        p = self.attention([p,p])\n",
        "        p = self.classifier(p)\n",
        "        \n",
        "        output = self.similarity([r, p])\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create embeddings"
      ],
      "metadata": {
        "id": "e_x7_DmlgJ7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = dict()\n",
        "f = open('data/glove.6B.50d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "2sTLEKPJgHRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(text_vocab), 50))\n",
        "for i, word in enumerate(text_vocab):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "Y1ee_sHSgcbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBDf-PB9_fxH"
      },
      "outputs": [],
      "source": [
        "token_embed = layers.Embedding(input_dim=len(text_vocab), output_dim=50, mask_zero=True, weights=[embedding_matrix], trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1e8CcrC_fxH"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCllRHiE_fxH"
      },
      "outputs": [],
      "source": [
        "model = TextCNN(token_embed, text_vectorizer, kernel_sizes=[2], dilation_rate=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps_rIklE_fxH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9\n",
        ")\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), metrics=[\"accuracy\"])\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "history_model_textcnn = model.fit(train_dataset, steps_per_epoch=len(train_dataset),\n",
        "                              epochs=50, \n",
        "                              validation_data=val_dataset, \n",
        "                              shuffle=True,\n",
        "                              validation_steps=len(val_dataset)\n",
        "                              )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "oj4eUaQjr7Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.ravel(model.predict((np.array(val_reviews),np.array(val_plots))))"
      ],
      "metadata": {
        "id": "eg1Nk_OVr2oB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_max = (y_pred>0.5)+0\n",
        "print(classification_report(val_labels, y_pred_max))"
      ],
      "metadata": {
        "id": "zBYx5XDMr9T0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf = confusion_matrix(val_labels, y_pred_max)\n",
        "print(cf)"
      ],
      "metadata": {
        "id": "_3vbltwwDp1X",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc(y_pred,y):\n",
        "    # generate a no skill prediction (majority class)\n",
        "    ns_probs = [0 for _ in range(len(y))]\n",
        "    \n",
        "    # predict probabilities\n",
        "    probs = y_pred\n",
        "    \n",
        "    # calculate scores\n",
        "    ns_auc = roc_auc_score(y, ns_probs)\n",
        "    auc = roc_auc_score(y, probs)\n",
        "    \n",
        "    # summarize scores\n",
        "    #print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
        "    print(f'ROC AUC=%.3f' % (auc))\n",
        "    \n",
        "    # calculate roc curves\n",
        "    ns_fpr, ns_tpr, _ = roc_curve(y, ns_probs)\n",
        "    fpr, tpr, _ = roc_curve(y, probs)\n",
        "    \n",
        "    # plot the roc curve for the model\n",
        "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random')\n",
        "    plt.plot(fpr, tpr, marker='.')\n",
        "    \n",
        "    # axis labels\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f\"ROC Curve (AUC=0.74)\")\n",
        "    \n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    # show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GQmaJ8KhOhe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc(y_pred, val_labels)"
      ],
      "metadata": {
        "id": "m1lTEJvdHMXN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oLQkQvIzHRQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "spoiler-detection-text-cnn-approach.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}